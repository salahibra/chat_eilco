version: '3.8'

services:
  fastapi:
    build: .
    container_name: chat-eilco-api
    ports:
      - "8000:8000"
    environment:
      - PYTHONUNBUFFERED=1
    env_file:
      - .env
    volumes:
      - ./data:/app/data
      - ./pdf_files:/app/pdf_files
    networks:
      - chat-network
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped
    # Uncomment if you want to run llama.cpp in Docker too
    # depends_on:
    #   - llama-cpp

  # Optional: Run llama.cpp in Docker
  # llama-cpp:
  #   image: ghcr.io/ggerganov/llama.cpp:latest
  #   container_name: llama-cpp-server
  #   ports:
  #     - "8080:8080"
  #   volumes:
  #     - ./models:/models
  #   command: 
  #     - "-m"
  #     - "/models/mistral-7b-instruct-v0.1.Q4_K_M.gguf"
  #     - "--server"
  #     - "-ngl"
  #     - "33"
  #     - "-c"
  #     - "2048"
  #   networks:
  #     - chat-network

networks:
  chat-network:
    driver: bridge
